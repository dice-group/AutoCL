# All of the best hyperparameter settings are shown here
### Best results per learning problem obtained from EvoLearner after HPO in terms of max_runtime, tournament_size, height_limit, card_limit, use_data_properties, use_inverse, quality_func, value_splitter, 𝐹1-measure and Accuracy. The first line of each learning problem represents the best hyperparameters and results before HPO, the second line represents the best hyperparameters and results after HPO. The results are the best among the 3 runs:
<img width="614" alt="Evolearner" src="https://github.com/dice-group/AutoCL/blob/main/HPO/Evolearner%20HPO.png">


After EvoLearner with HPO which shown in Table 8, we can observe an improvement in quality scores on Carcinogenesis, Hepatitis, Mammographic, and Pyrimidine. In particular, for dataset Carcinogenesis, the 𝐹1-measure improved by 6% and 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 improved by 10% after HPO. Similarly, excellent performance is also reflected in Pyrimidine: the 𝐹1-measure is almost 16% better. Family, NCTRER, Mutagenesis and Premier League keep the best 𝐹1-measure and 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 before and after HPO, after some values of hyperparameter changed.
Then we continue to analyse the changes in scoring criteria in conjunction with the value of the hyperparameter. From this table, it can be observed that the optimal running time of all datasets is much smaller than the original, which proves that HPO accelerates the learning time of the concept learner. Some datasets like Carcinogenesis, Hepatitis, the best value of hyperparameter including the tournament_size,height-limit,card-limit are different from the original default values. In the evolutionary algorithm EvoLearner, all of these values have an effect on each other, and when the number of tournaments becomes more or less, it will affect the algorithm’s decision-making over include.
A more satisfactory result is obtained when the default value of 𝑇𝑟𝑢𝑒 is selected for use_data_properties has been shown in EvoLearner. At the same time, the hyperparameter value_splitter only works when use_data_properties was set to 𝑇𝑟𝑢𝑒. Furthermore, we find that different values of this value_splitter that appear in the optimum parameters may not greatly influence our result. For the hyperparameter quality_func, the 𝐹1 appears same times with 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦. Lastly, the value of use_inverse, 𝑇𝑟𝑢𝑒 and 𝐹𝑎𝑙𝑠𝑒 shown half.

### Best results per learning problem obtained from OCEL after HPO in terms of max_runtime in seconds, max_num_of_concepts_tested,iter_bound,quality_func, 𝐹1_measure and Accuracy. The first line of each learning problem represents the best hyperparameters and results before HPO, the second line represents the best hyperparameters and results after HPO. The results are the best among the 3 runs：
<img width="614" alt="Ocel" src="https://github.com/AutoCL2023/AutoCL/blob/main/OCEL%20HPO.png">

In OCEL with HPO, we can find that the optimal value of max-run-time is reduced on all datasets, and the 𝐹1-measure as well as the 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 is improved on four of them, especially for the large dataset with more atoms Premier League, where the 𝐹1-measure and 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 are improved by at least 50%.
On other parameters such as max_num_of_concepts_tested, iter_bound, the optimal values are also very different from the original defaults, and the reduction in these values means that the concept learner is able to use a narrower learning range to get the results we want.

### Best results per learning problem obtained from CELOE after HPO in terms of max_runtime in seconds, max_num_of_concepts_tested,iter_bound,quality_func, 𝐹1_measure and Accuracy. The first line of each learning problem represents the best hyperparameters and results before HPO, the second line represents the best hyperparameters and results after HPO. The results are the best among the 3 runs：
<img width="614" alt="Celoe" src="https://github.com/AutoCL2023/AutoCL/blob/main/CELOE%20HPO.png">
When HPO was applied to CELOE, it produced similar results as on OCEL: the conceptual learner improved learn- ing performance on more than half of the datasets while reducing the runtime and learning range. On the Carcinogenesis, Famliy datasets, 𝐹1-measure, and 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 still produced quality scores similar to the original data.
